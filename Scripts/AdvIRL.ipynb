{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 20:48:32.803112: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-05 20:48:33.411706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import msgpack\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from scenes import *\n",
    "import pyngp as ngp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ../scripts/run.py --scene=../BananaScene --save_screenshot=\"../SaveModel.msgpack\" --n_steps=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loaded = None\n",
    "# parameters = None\n",
    "# with open(\"../SavedModel.msgpack\", \"rb\") as data_file:\n",
    "#     byte_data = data_file.read()\n",
    "#     data_loaded = msgpack.unpackb(byte_data)\n",
    "#     print(data_loaded['snapshot']['render_aabb'])\n",
    "#     parameters = np.frombuffer(data_loaded[\"snapshot\"][\"params_binary\"], dtype=np.float16).copy()\n",
    "#     print(len(parameters.tobytes()))\n",
    "#     # for i in range(100):\n",
    "#     #     # if i > 20:\n",
    "#     #     #     break\n",
    "#     #     # print(test[i])\n",
    "#     #     randNum = random.randint(-5, 5)\n",
    "#     #     test[i] += randNum\n",
    "#     # print(len(test))\n",
    "#     # print(data_loaded['snapshot']['n_params'])\n",
    "#     # print(test.max())\n",
    "#     # print(test.min())\n",
    "#     # data_loaded[\"snapshot\"][\"params_binary\"] = test.tobytes()\n",
    "#     # with open(\"../Test2.msgpack\", \"wb\") as file:\n",
    "#     #     file.write(msgpack.packb(data_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array = None\n",
    "# data_loaded = None\n",
    "# with open(\"../SavedModel.msgpack\", \"rb\") as data_file:\n",
    "#     byte_data = data_file.read()\n",
    "#     data_loaded = msgpack.unpackb(byte_data)\n",
    "#     array = bytearray(data_loaded['snapshot']['params_binary'])\n",
    "#     # TODO: So we're modifying the full feature grid?\n",
    "#     for i in range(10000):\n",
    "#         randNum = random.randint(1, 5)\n",
    "#         if array[i] + randNum < 256:\n",
    "#             array[i] += randNum\n",
    "    \n",
    "#     data_loaded['snapshot']['params_binary'] = bytes(array)\n",
    "\n",
    "#     with open(\"../Test.msgpack\", \"wb\") as file:\n",
    "#         file.write(msgpack.packb(data_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene(scene):\n",
    "\tfor scenes in [scenes_sdf, scenes_nerf, scenes_image, scenes_volume]:\n",
    "\t\tif scene in scenes:\n",
    "\t\t\treturn scenes[scene]\n",
    "\treturn None\n",
    "\n",
    "def Train_NeRF(testbed, load_snapshot, screenshot_transforms, screenshot_dir, sharpen=0, exposure=0.0, screenshot_frames=None, screenshot_spp=16, width=224, height=224, network=None):\n",
    "\n",
    "\tif load_snapshot:\n",
    "\t\tscene_info = get_scene(load_snapshot)\n",
    "\t\tif scene_info is not None:\n",
    "\t\t\tload_snapshot = default_snapshot_filename(scene_info)\n",
    "\t\ttestbed.load_snapshot(load_snapshot)\n",
    "\n",
    "\tref_transforms = {}\n",
    "\tif screenshot_transforms: # try to load the given file straight away\n",
    "\t\tprint(\"Screenshot transforms from \", screenshot_transforms)\n",
    "\t\twith open(screenshot_transforms) as f:\n",
    "\t\t\tref_transforms = json.load(f)\n",
    "\n",
    "\tif testbed.mode == ngp.TestbedMode.Sdf:\n",
    "\t\ttestbed.tonemap_curve = ngp.TonemapCurve.ACES\n",
    "\n",
    "\ttestbed.nerf.sharpen = float(sharpen)\n",
    "\ttestbed.exposure = exposure\n",
    "\ttestbed.shall_train = True\n",
    "\n",
    "\n",
    "\ttestbed.nerf.render_with_lens_distortion = True\n",
    "\n",
    "\tif ref_transforms:\n",
    "\t\ttestbed.fov_axis = 0\n",
    "\t\ttestbed.fov = ref_transforms[\"camera_angle_x\"] * 180 / np.pi\n",
    "\t\tif not screenshot_frames:\n",
    "\t\t\tscreenshot_frames = range(len(ref_transforms[\"frames\"]))\n",
    "\n",
    "\t\tfor idx in screenshot_frames:\n",
    "\t\t\tf = ref_transforms[\"frames\"][int(idx)]\n",
    "\t\t\tif 'transform_matrix' in f:\n",
    "\t\t\t\tcam_matrix = f['transform_matrix']\n",
    "\t\t\telif 'transform_matrix_start' in f:\n",
    "\t\t\t\tcam_matrix = f[\"transform_matrix_start\"]\n",
    "\t\t\telse:\n",
    "\t\t\t\traise KeyError()\n",
    "\t\t\ttestbed.set_nerf_camera_matrix(np.matrix(cam_matrix)[:-1,:])\n",
    "\t\t\toutname = os.path.join(screenshot_dir, os.path.basename(f[\"file_path\"]))\n",
    "\n",
    "\t\t\t# Some NeRF datasets lack the .png suffix in the dataset metadata\n",
    "\t\t\tif not os.path.splitext(outname)[1]:\n",
    "\t\t\t\toutname = outname + \".png\"\n",
    "\n",
    "\t\t\timage = testbed.render(width or int(ref_transforms[\"w\"]), height or int(ref_transforms[\"h\"]), screenshot_spp, True)\n",
    "\t\t\tos.makedirs(os.path.dirname(outname), exist_ok=True)\n",
    "\t\t\twrite_image(outname, image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNeRFData(fileName = \"../SavedModel.msgpack\"):\n",
    "    with open(fileName, \"rb\") as data_file:\n",
    "        byte_data = data_file.read()\n",
    "        data_loaded = msgpack.unpackb(byte_data)\n",
    "        parameters = np.frombuffer(data_loaded[\"snapshot\"][\"params_binary\"], dtype=np.float16).copy()\n",
    "        return data_loaded, parameters\n",
    "        \n",
    "def SaveParameters(data_loaded, parameters, outputFileName = \"../Test.msgpack\"):\n",
    "    data_loaded['snapshot']['params_binary'] = parameters.tobytes()\n",
    "\n",
    "    with open(outputFileName, \"wb\") as file:\n",
    "        file.write(msgpack.packb(data_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 20:48:34.543058: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:1000] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-05 20:48:34.543686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:1000] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-05 20:48:34.588769: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2252] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = tf.keras.applications.MobileNetV2()\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('n01945685', 'slug', 0.6970356),\n",
       "  ('n07753592', 'banana', 0.03878794),\n",
       "  ('n02264363', 'lacewing', 0.028227882),\n",
       "  ('n01930112', 'nematode', 0.026499515),\n",
       "  ('n01728572', 'thunder_snake', 0.010387602)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium import spaces\n",
    "# import numpy as np\n",
    "# from stable_baselines3 import PPO, A2C\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# from PIL import Image\n",
    "# from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "\n",
    "# # Preprocess numpy images\n",
    "# def preprocess(image, unproc=False):\n",
    "#     if not unproc:\n",
    "#         image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "#         return np.asarray(image, dtype=np.float32)\n",
    "#     else:\n",
    "#         return np.asarray(image, dtype=np.uint8)\n",
    "#     # Iteration ste\n",
    "\n",
    "# # Loads images from path\n",
    "# def prep_im(image_path, unproc=False):\n",
    "#     image = np.asarray(Image.open(image_path), dtype=np.float32)\n",
    "#     # ## print(image_raw.shape())\n",
    "#     image = image[None, ...]\n",
    "#     image = preprocess(image, unproc=unproc)\n",
    "#     #     image_probs = pretrained_model.predict(image)\n",
    "#     #     plot_fig(image, image_probs)\n",
    "#     return image\n",
    "\n",
    "# pred = pretrained_model.predict(prep_im(\"../../Slug69Percent.jpg\"))\n",
    "# decode_predictions(pred, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m20:48:35 \u001b[0;32mSUCCESS  \u001b[0mInitialized CUDA 11.8. Active GPU is #1: NVIDIA GeForce RTX 3090 [86]\u001b[K\u001b[0m\n",
      "20:48:35 \u001b[0;32mSUCCESS  \u001b[0mDetected auxiliary GPUs:\u001b[K\u001b[0m\n",
      "20:48:35 \u001b[0;32mSUCCESS  \u001b[0m  #0: NVIDIA GeForce RTX 3090 [86]\u001b[K\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "testbed = ngp.Testbed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:49:38 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:49:38 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:49:38 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:49:38 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:49:38 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 559ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.32665408)]]\n",
      "Psnr: 2.151578353557924, MSE: tf.Tensor(0.003235368, shape=(), dtype=float32) for class slug\n",
      "saving tape...\n",
      "Current episode reward: tf.Tensor(1.7614489, shape=(), dtype=float32)\n",
      "20:50:04 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:50:04 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:50:04 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:50:04 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:50:04 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.16279581)]]\n",
      "Psnr: 1.8205994772695078, MSE: tf.Tensor(0.014855301, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(2.665709, shape=(), dtype=float32)\n",
      "20:50:28 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:50:28 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:50:28 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:50:28 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:50:28 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.31069997)]]\n",
      "Psnr: 2.1517641749691236, MSE: tf.Tensor(0.0032326006, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(4.220499, shape=(), dtype=float32)\n",
      "20:50:54 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:50:54 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:50:54 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:50:54 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:50:54 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.36715776)]]\n",
      "Psnr: 3.4946054101356894, MSE: tf.Tensor(6.666065e-06, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-99.45108, shape=(), dtype=float32)\n",
      "20:51:20 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:51:21 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:51:21 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:51:21 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:51:21 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.39062557)]]\n",
      "Psnr: 2.2063973955008906, MSE: tf.Tensor(0.002513538, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-203.35733, shape=(), dtype=float32)\n",
      "20:51:48 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:51:49 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:51:49 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:51:49 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:51:49 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.31271213)]]\n",
      "Psnr: 1.940746381890611, MSE: tf.Tensor(0.008542552, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-202.33484, shape=(), dtype=float32)\n",
      "20:52:17 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:52:17 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:52:17 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:52:17 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:52:17 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.300805)]]\n",
      "Psnr: 1.7945636980928696, MSE: tf.Tensor(0.016747616, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-305.3429, shape=(), dtype=float32)\n",
      "20:52:46 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:52:47 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:52:47 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:52:47 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:52:47 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.31848663)]]\n",
      "Psnr: 1.940790670155274, MSE: tf.Tensor(0.008540809, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-408.52777, shape=(), dtype=float32)\n",
      "20:53:15 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:53:15 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:53:15 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:53:15 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:53:15 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.39029336)]]\n",
      "Psnr: 2.2064265943285113, MSE: tf.Tensor(0.0025132, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-512.4307, shape=(), dtype=float32)\n",
      "20:53:43 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:53:43 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:53:43 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:53:43 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:53:43 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.31848663)]]\n",
      "Psnr: 1.940790670155274, MSE: tf.Tensor(0.008540809, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-615.6156, shape=(), dtype=float32)\n",
      "20:54:11 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:54:12 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:54:12 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:54:12 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:54:12 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.300805)]]\n",
      "Psnr: 1.7945636980928696, MSE: tf.Tensor(0.016747616, shape=(), dtype=float32) for class banana\n",
      "saving tape...\n",
      "Current episode reward: tf.Tensor(-718.62366, shape=(), dtype=float32)\n",
      "20:54:41 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:54:42 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:54:42 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:54:42 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:54:42 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.29003343)]]\n",
      "Psnr: 1.6985013096763932, MSE: tf.Tensor(0.026066197, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-717.2902, shape=(), dtype=float32)\n",
      "20:55:12 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:55:13 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:55:13 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:55:13 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:55:13 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.3082292)]]\n",
      "Psnr: 1.7946036163447978, MSE: tf.Tensor(0.01674454, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-820.3725, shape=(), dtype=float32)\n",
      "20:55:43 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:55:43 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:55:43 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:55:43 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:55:43 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.32207835)]]\n",
      "Psnr: 1.940814142542336, MSE: tf.Tensor(0.008539886, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-923.59326, shape=(), dtype=float32)\n",
      "20:56:12 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:56:12 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:56:12 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:56:12 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:56:12 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.39059523)]]\n",
      "Psnr: 2.2066197516425037, MSE: tf.Tensor(0.0025109656, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-1027.4993, shape=(), dtype=float32)\n",
      "20:56:40 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:56:40 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:56:40 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:56:40 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:56:40 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.3427208)]]\n",
      "Psnr: 3.519980416637331, MSE: tf.Tensor(5.9308854e-06, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-1130.9265, shape=(), dtype=float32)\n",
      "20:57:07 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:57:07 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:57:07 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:57:07 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:57:07 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.39059523)]]\n",
      "Psnr: 2.2066197516425037, MSE: tf.Tensor(0.0025109656, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-1234.8325, shape=(), dtype=float32)\n",
      "20:57:35 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:57:36 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:57:36 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:57:36 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:57:36 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.32207835)]]\n",
      "Psnr: 1.940814142542336, MSE: tf.Tensor(0.008539886, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-1338.0533, shape=(), dtype=float32)\n",
      "20:58:04 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:58:05 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:58:05 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:58:05 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:58:05 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.3082292)]]\n",
      "Psnr: 1.7946036163447978, MSE: tf.Tensor(0.01674454, shape=(), dtype=float32) for class banana\n",
      "Current episode reward: tf.Tensor(-1441.1356, shape=(), dtype=float32)\n",
      "20:58:35 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:58:35 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:58:35 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:58:35 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:58:35 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.29003343)]]\n",
      "Psnr: 1.6985013096763932, MSE: tf.Tensor(0.026066197, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1439.8022, shape=(), dtype=float32)\n",
      "20:59:06 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:59:06 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:59:06 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:59:06 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:59:06 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.3082292)]]\n",
      "Psnr: 1.7946036163447978, MSE: tf.Tensor(0.01674454, shape=(), dtype=float32) for class banana\n",
      "saving tape...\n",
      "Current episode reward: tf.Tensor(-1542.8845, shape=(), dtype=float32)\n",
      "20:59:36 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "20:59:36 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "20:59:36 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "20:59:36 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "20:59:36 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.29003343)]]\n",
      "Psnr: 1.6985013096763932, MSE: tf.Tensor(0.026066197, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1541.5511, shape=(), dtype=float32)\n",
      "21:00:07 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:00:07 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:00:07 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:00:07 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:00:07 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.26703477)]]\n",
      "Psnr: 1.6288582900379325, MSE: tf.Tensor(0.035922267, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1539.6119, shape=(), dtype=float32)\n",
      "21:00:37 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:00:37 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:00:37 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:00:37 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:00:37 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.30725247)]]\n",
      "Psnr: 1.698577823135682, MSE: tf.Tensor(0.026057014, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1538.1346, shape=(), dtype=float32)\n",
      "21:01:07 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:01:07 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:01:07 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:01:07 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:01:07 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.26703477)]]\n",
      "Psnr: 1.6288582900379325, MSE: tf.Tensor(0.035922267, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1536.1954, shape=(), dtype=float32)\n",
      "21:01:37 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:01:37 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:01:37 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:01:37 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:01:37 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.36850375)]]\n",
      "Psnr: 1.5723961398686264, MSE: tf.Tensor(0.046589505, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1532.526, shape=(), dtype=float32)\n",
      "21:02:08 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:02:09 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:02:09 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:02:09 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:02:09 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.27165243)]]\n",
      "Psnr: 1.628853174068968, MSE: tf.Tensor(0.035923116, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1530.501, shape=(), dtype=float32)\n",
      "Screenshot transforms from 21:02:39 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      " ../BananaScene/transforms.json\n",
      "21:02:39 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:02:39 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:02:39 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:02:39 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.36850375)]]\n",
      "Psnr: 1.5723961398686264, MSE: tf.Tensor(0.046589505, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1526.8315, shape=(), dtype=float32)\n",
      "21:03:10 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:03:10 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:03:10 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:03:10 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:03:10 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.27165243)]]\n",
      "Psnr: 1.628853174068968, MSE: tf.Tensor(0.035923116, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1524.8065, shape=(), dtype=float32)\n",
      "21:03:40 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:03:40 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:03:40 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:03:40 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:03:40 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.31517372)]]\n",
      "Psnr: 1.6985836297312376, MSE: tf.Tensor(0.02605632, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1523.1637, shape=(), dtype=float32)\n",
      "21:04:09 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:04:10 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:04:10 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:04:10 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:04:10 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Top prediction:  [[('n07753592', 'banana', 0.29505852)]]\n",
      "Psnr: 1.794665190590861, MSE: tf.Tensor(0.016739791, shape=(), dtype=float32) for class banana\n",
      "saving tape...\n",
      "Current episode reward: tf.Tensor(-1626.1143, shape=(), dtype=float32)\n",
      "21:04:38 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:04:38 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:04:38 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:04:38 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:04:38 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.31517372)]]\n",
      "Psnr: 1.6985836297312376, MSE: tf.Tensor(0.02605632, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1624.4714, shape=(), dtype=float32)\n",
      "21:05:08 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:05:08 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:05:08 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:05:08 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:05:08 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.27165243)]]\n",
      "Psnr: 1.628853174068968, MSE: tf.Tensor(0.035923116, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1622.4464, shape=(), dtype=float32)\n",
      "21:05:38 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:05:38 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:05:38 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:05:38 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:05:38 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.31517372)]]\n",
      "Psnr: 1.6985836297312376, MSE: tf.Tensor(0.02605632, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1620.8036, shape=(), dtype=float32)\n",
      "21:06:08 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:06:08 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:06:08 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:06:08 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:06:08 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.27165243)]]\n",
      "Psnr: 1.628853174068968, MSE: tf.Tensor(0.035923116, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1618.7786, shape=(), dtype=float32)\n",
      "21:06:38 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:06:38 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:06:38 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:06:38 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:06:38 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.36850375)]]\n",
      "Psnr: 1.5723961398686264, MSE: tf.Tensor(0.046589505, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1615.1091, shape=(), dtype=float32)\n",
      "21:07:09 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:07:09 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:07:09 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:07:09 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:07:09 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.42031908)]]\n",
      "Psnr: 1.5242717340993455, MSE: tf.Tensor(0.058148287, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1611.0883, shape=(), dtype=float32)\n",
      "21:07:41 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:07:41 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:07:41 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:07:41 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:07:41 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.43821603)]]\n",
      "Psnr: 1.4805907369020652, MSE: tf.Tensor(0.0711048, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1606.6057, shape=(), dtype=float32)\n",
      "21:08:13 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:08:14 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:08:14 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:08:14 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:08:14 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.39693707)]]\n",
      "Psnr: 1.439596755266598, MSE: tf.Tensor(0.085878976, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1602.6367, shape=(), dtype=float32)\n",
      "21:08:47 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:08:47 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:08:47 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:08:47 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:08:47 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.45258868)]]\n",
      "Psnr: 1.3990200475459154, MSE: tf.Tensor(0.10352382, shape=(), dtype=float32) for class slug\n",
      "Current episode reward: tf.Tensor(-1598.2916, shape=(), dtype=float32)\n",
      "21:09:20 \u001b[0;36mINFO     \u001b[0mLoading network snapshot from: ../Test.msgpack\u001b[K\u001b[0m\n",
      "Screenshot transforms from  ../BananaScene/transforms.json\n",
      "21:09:21 \u001b[0;36mINFO     \u001b[0mGridEncoding:  Nmin=16 b=3.28134 F=4 T=2^19 L=8\u001b[K\u001b[0m\n",
      "21:09:21 \u001b[0;36mINFO     \u001b[0mDensity model: 3--[HashGrid]-->32--[FullyFusedMLP(neurons=64,layers=3)]-->1\u001b[K\u001b[0m\n",
      "21:09:21 \u001b[0;36mINFO     \u001b[0mColor model:   3--[Composite]-->16+16--[FullyFusedMLP(neurons=64,layers=4)]-->3\u001b[K\u001b[0m\n",
      "21:09:21 \u001b[0;36mINFO     \u001b[0m  total_encoding_params=13194816 total_network_params=10240\u001b[K\u001b[0m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Top prediction:  [[('n01945685', 'slug', 0.6970356)]]\n",
      "Psnr: 1.359817719626236, MSE: tf.Tensor(0.12400662, shape=(), dtype=float32) for class slug\n",
      "saving tape...\n",
      "Current episode reward: tf.Tensor(-1591.2772, shape=(), dtype=float32)\n",
      "Episode reward: tf.Tensor(-1591.2772, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from PIL import Image\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "logPath = '../AdvOutput/rewards.log'\n",
    "# Preprocess numpy images\n",
    "def preprocess(image, unproc=False):\n",
    "    if not unproc:\n",
    "        image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "        return np.asarray(image, dtype=np.float32)\n",
    "    else:\n",
    "        return np.asarray(image, dtype=np.uint8)\n",
    "    # Iteration ste\n",
    "\n",
    "# Loads images from path\n",
    "def prep_im(image_path, unproc=False):\n",
    "    image = np.asarray(Image.open(image_path), dtype=np.float32)\n",
    "    # ## print(image_raw.shape())\n",
    "    image = image[None, ...]\n",
    "    image = preprocess(image, unproc=unproc)\n",
    "    #     image_probs = pretrained_model.predict(image)\n",
    "    #     plot_fig(image, image_probs)\n",
    "    return image\n",
    "\n",
    "class InstantNGPEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(InstantNGPEnv, self).__init__()\n",
    "        # self.reward_range = (-1,1)\n",
    "        # Load Instant-NGP and MobileNetV2 models\n",
    "        # self.instant_ngp = ...  # Load Instant-NGP model\n",
    "        self.data_loaded, self.feature_grid = loadNeRFData()\n",
    "        self.ground_truth_img = None\n",
    "        self.log = open(logPath, 'a')\n",
    "\n",
    "\n",
    "        self.mobilenetv2 = pretrained_model  # Load MobileNetV2 model\n",
    "        self.true_class_id = 954\n",
    "        self.target = \"slug\" # Target tiger shark class\n",
    "        self.noiseAmount = 0.15\n",
    "        self.max_reward = -np.inf\n",
    "        self.epochs = 0\n",
    "        self.targeted_grad = None\n",
    "        self.unrendered = None\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(low=-0.005, high=0.005, shape=(13205056,), dtype=np.float32)  # Define space of allowable feature grid modifications\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(13205056,), dtype=np.float32) # Define state representation (e.g., feature grid, image)\n",
    "\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "        # Reset feature grid or load new one\n",
    "        self.mobilenetv2.trainable = False\n",
    "        self.ground_truth_img = prep_im(\"../NormalOutput/0081.jpg\")\n",
    "        self.data_loaded, self.feature_grid = loadNeRFData()\n",
    "        # self.log.close()\n",
    "        return (self.feature_grid, {})\n",
    "    \n",
    "    def get_reward(self, originalImage, renderedImage, predicted, min_psnr=30, max_psnr=50, mse_weight = -2, psnr_weight = .5):\n",
    "\n",
    "        returned_reward = 0\n",
    "        imageLoss = mse(originalImage, renderedImage)\n",
    "        psnr = (peak_signal_noise_ratio(originalImage, renderedImage, data_range=255) - min_psnr) / (max_psnr - min_psnr)\n",
    "        if predicted[1] == self.target:\n",
    "            returned_reward = predicted[2] * 10 + psnr * psnr_weight + imageLoss * mse_weight\n",
    "        else:\n",
    "            returned_reward = -100\n",
    "\n",
    "        print(\"Psnr: \" + str(psnr) + \", MSE: \" + str(imageLoss) + \" for class \" + predicted[1])\n",
    "        return returned_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        # Modify feature grid based on action\n",
    "        self.feature_grid +=  action[0] # * self.noiseAmount  # Apply action to feature grid\n",
    "\n",
    "        # Render image from modified feature grid\n",
    "        SaveParameters(self.data_loaded, self.feature_grid)\n",
    "        Train_NeRF(testbed, \"../Test.msgpack\", \"../BananaScene/transforms.json\", \"../AdvOutput/\")\n",
    "        image = prep_im(\"../AdvOutput/0081.jpg\")\n",
    "\n",
    "        # Classify image using MobileNetV2\n",
    "        prediction = self.mobilenetv2.predict(image)\n",
    "\n",
    "        print(\"Top prediction:  \" + str(decode_predictions(prediction, top=1)))\n",
    "\n",
    "        decoded_prediction = decode_predictions(prediction, top=1)[0][0]\n",
    "        # Calculate reward based on adversarial success and image quality\n",
    "        reward = self.get_reward(self.ground_truth_img, image, decoded_prediction)  # Implement reward function\n",
    "        # reward += (1 - (np.squeeze(prediction)[self.true_class_id])) # TODO: Adjust reward so model gets to slug with high accuracy.\n",
    "        reward -= np.squeeze(prediction)[self.true_class_id] * 10\n",
    "        # print(\"Banana: \" + str(decode_predictions(prediction)) + \", conf: \" + str(np.squeeze(prediction)[self.true_class_id]))\n",
    "        # Determine if episode is done\n",
    "        done = False\n",
    "\n",
    "        # TODO: The reason why it keeps predicting the same thing has to do with this \"done\" flag and its criteria. Find another criteria.\n",
    "        # done = True if reward <= -100 else False  # Define termination criteria\n",
    "        truncated, info = False, {}\n",
    "        \n",
    "        if self.epochs % 10 == 0 or reward == self.max_reward:\n",
    "        # if self.epochs % 250 == 0:\n",
    "            print('saving tape...')\n",
    "            self.log.write(\n",
    "                '\\n'\n",
    "                + str(self.epochs)\n",
    "                + ', '\n",
    "                + str(decode_predictions(prediction, top=1))\n",
    "            )\n",
    "        self.epochs += 1\n",
    "\n",
    "        info = {\n",
    "            \"Target\" : self.target,\n",
    "            \"Predicted\" : decoded_prediction[1],\n",
    "            \"Confidence\" : decoded_prediction[2],\n",
    "            \"Epochs\" : self.epochs\n",
    "        }\n",
    "\n",
    "        return self.feature_grid, reward, done, truncated, info\n",
    "\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "# Create environment instance\n",
    "env = InstantNGPEnv()\n",
    "\n",
    "# Create RL agent\n",
    "model = PPO('MlpPolicy', env, device='cuda')\n",
    "\n",
    "# Train the agent\n",
    "# Evaluate the trained agent\n",
    "episode_reward = 0\n",
    "obs, info = env.reset()\n",
    "_states = None\n",
    "# TODO: Once done testing, comment out line below\n",
    "# model = model.learn(total_timesteps=2000)\n",
    "\n",
    "for i in range(2000):\n",
    "    action, _states = model.predict(obs, _states)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    print(\"Current episode reward: \" + str(episode_reward))\n",
    "    \n",
    "\n",
    "    if info['Target'] == info['Predicted'] and info['Confidence'] >= .65:\n",
    "        print(\"Episode reward:\", episode_reward)\n",
    "        break\n",
    "    # model = model.learn(total_timesteps=1, log_interval=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python adversarialAttack.py --load_snapshot=../SavedModel.msgpack --screenshot_transforms=../BananaScene/transforms.json --screenshot_dir=../NormalOutput --width=224 --height=224\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained ImageNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InstantNGPENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
